{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM to Seq2Seq\n",
    "- I will try to use medical examples and apply lstm or seq2seq\n",
    "\n",
    "# LSTM\n",
    "- lstm cell\n",
    "\n",
    "![lstm](https://miro.medium.com/max/900/1*s7_EO0rjXAw99RnH1x4s_g.png)\n",
    "- lstm cell takes 3 input\n",
    "    - cell state from $t-1$\n",
    "    - hidden state from $t-1$\n",
    "    - current input $x_t$\n",
    "\n",
    "- lstm cell output \n",
    "    - cell state from $t$\n",
    "    - hidden state from $t$\n",
    "    - output from $t$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "use sample data\n",
    "\n",
    "input : length 1 with 10 dimension\\\n",
    "output : length 1 20 dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn = nn.LSTM(10, 20, 1) # (input_size, hidden_size, num_layers)\n",
    "input = torch.randn(1, 1, 10) # (Series Length, Batch size, input_dim)\n",
    "h0 = torch.randn(1, 1, 20) # (Series Length, Batch size, input_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 20, 1)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn.input_size, rnn.hidden_size, rnn.num_layers\n",
    "# input size , hidden size,  number of layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = rnn(input)\n",
    "\n",
    "len(output) # length of 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[-0.0511,  0.0860, -0.1217,  0.0877,  0.0426,  0.0022,  0.0229,\n",
       "           -0.0829, -0.0583, -0.0324, -0.1867,  0.1178,  0.0094,  0.0187,\n",
       "            0.0352,  0.0582, -0.0778,  0.0369,  0.0825, -0.0036]]],\n",
       "        grad_fn=<StackBackward0>),\n",
       " (tensor([[[-0.0511,  0.0860, -0.1217,  0.0877,  0.0426,  0.0022,  0.0229,\n",
       "            -0.0829, -0.0583, -0.0324, -0.1867,  0.1178,  0.0094,  0.0187,\n",
       "             0.0352,  0.0582, -0.0778,  0.0369,  0.0825, -0.0036]]],\n",
       "         grad_fn=<StackBackward0>),\n",
       "  tensor([[[-0.1416,  0.1849, -0.2309,  0.1433,  0.0767,  0.0040,  0.0547,\n",
       "            -0.1416, -0.1010, -0.0884, -0.4111,  0.2050,  0.0194,  0.0476,\n",
       "             0.0682,  0.0931, -0.1866,  0.0725,  0.1666, -0.0062]]],\n",
       "         grad_fn=<StackBackward0>)))"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output[0], output[1]\n",
    "\n",
    "# output 0 : 1,1,20 vector. output of layer\n",
    "# output 1 : (output and the hidden output & current state output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if we changed the sequence of the vector to 2?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = torch.rand(2, 1, 10) # sequence length 2, batch_size, 1, embedding size 10\n",
    "rnn = nn.LSTM(10, 20, 1) # same model as before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[-0.0569,  0.1192, -0.0259,  0.0212, -0.0842, -0.0415, -0.1121,\n",
       "            0.0661, -0.0783,  0.0848, -0.0500, -0.0435,  0.0087,  0.0822,\n",
       "           -0.0447,  0.1175, -0.0532, -0.1104,  0.0928, -0.0492]],\n",
       " \n",
       "         [[-0.0831,  0.1595, -0.0117,  0.0178, -0.1352, -0.0932, -0.1217,\n",
       "            0.1179, -0.1499,  0.1582, -0.0862, -0.0483, -0.0161,  0.1349,\n",
       "            0.0220,  0.1528, -0.0120, -0.1601,  0.1565, -0.0897]]],\n",
       "        grad_fn=<StackBackward0>),\n",
       " (tensor([[[-0.0831,  0.1595, -0.0117,  0.0178, -0.1352, -0.0932, -0.1217,\n",
       "             0.1179, -0.1499,  0.1582, -0.0862, -0.0483, -0.0161,  0.1349,\n",
       "             0.0220,  0.1528, -0.0120, -0.1601,  0.1565, -0.0897]]],\n",
       "         grad_fn=<StackBackward0>),\n",
       "  tensor([[[-0.1749,  0.3374, -0.0240,  0.0464, -0.3265, -0.1827, -0.2405,\n",
       "             0.2857, -0.3497,  0.3893, -0.1783, -0.0808, -0.0323,  0.2939,\n",
       "             0.0380,  0.2307, -0.0278, -0.4370,  0.2883, -0.2168]]],\n",
       "         grad_fn=<StackBackward0>)))"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = rnn(input) \n",
    "\n",
    "output[0], output[1] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1, 20])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output[0].shape \n",
    "# The hidden state output. \n",
    "\n",
    "# (Length 2, batch, embed_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[-0.0831,  0.1595, -0.0117,  0.0178, -0.1352, -0.0932, -0.1217,\n",
       "            0.1179, -0.1499,  0.1582, -0.0862, -0.0483, -0.0161,  0.1349,\n",
       "            0.0220,  0.1528, -0.0120, -0.1601,  0.1565, -0.0897]]],\n",
       "        grad_fn=<StackBackward0>),\n",
       " tensor([[[-0.1749,  0.3374, -0.0240,  0.0464, -0.3265, -0.1827, -0.2405,\n",
       "            0.2857, -0.3497,  0.3893, -0.1783, -0.0808, -0.0323,  0.2939,\n",
       "            0.0380,  0.2307, -0.0278, -0.4370,  0.2883, -0.2168]]],\n",
       "        grad_fn=<StackBackward0>))"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output[1]\n",
    "\n",
    "# (hidden state : (sequence_size, batch_size, embedding), current_state : (sequence_size, batch_size, embedding))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try out bi-lstm model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "bi_lstm = nn.LSTM(10, 20, 1, bidirectional=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = bi_lstm(input) # input : length 2, 1 batch, size 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1, 40])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output[0].shape \n",
    "\n",
    "# length 2, batch 1, 2 of 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The only difference is that the embedding becomes twice the size of the hidden output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer1 = nn.LSTM(10,20,2,bidirectional=True)\n",
    "layer2 = nn.LSTM(40,5,2,bidirectional=True)\n",
    "\n",
    "class extract_tensor(nn.Module):\n",
    "    def forward(self, x):\n",
    "        tensor, _ = x\n",
    "        return tensor\n",
    "\n",
    "model1 = nn.Sequential(\n",
    "    layer1,\n",
    "    extract_tensor(),\n",
    "    layer2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0613, -0.1756, -0.0847, -0.0718, -0.1153]],\n",
       "\n",
       "        [[ 0.1223, -0.0876,  0.0668, -0.0772,  0.0308]],\n",
       "\n",
       "        [[-0.1749, -0.0881, -0.2084, -0.1275, -0.1554]],\n",
       "\n",
       "        [[ 0.0212, -0.0551, -0.0744,  0.0346, -0.0522]]],\n",
       "       grad_fn=<StackBackward0>)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1(input)[1][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bi-LSTM models with some dataset\n",
    "- dataset : Tabular data - titanic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "project_dir = Path.cwd().parent\n",
    "data_dir = project_dir.joinpath('data')\n",
    "\n",
    "data = data_dir.joinpath('train.csv')\n",
    "data = pd.read_csv(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C85</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113803</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>C123</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Allen, Mr. William Henry</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>373450</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Survived  Pclass  \\\n",
       "0            1         0       3   \n",
       "1            2         1       1   \n",
       "2            3         1       3   \n",
       "3            4         1       1   \n",
       "4            5         0       3   \n",
       "\n",
       "                                                Name     Sex   Age  SibSp  \\\n",
       "0                            Braund, Mr. Owen Harris    male  22.0      1   \n",
       "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
       "2                             Heikkinen, Miss. Laina  female  26.0      0   \n",
       "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
       "4                           Allen, Mr. William Henry    male  35.0      0   \n",
       "\n",
       "   Parch            Ticket     Fare Cabin Embarked  \n",
       "0      0         A/5 21171   7.2500   NaN        S  \n",
       "1      0          PC 17599  71.2833   C85        C  \n",
       "2      0  STON/O2. 3101282   7.9250   NaN        S  \n",
       "3      0            113803  53.1000  C123        S  \n",
       "4      0            373450   8.0500   NaN        S  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wonseok/.pyenv/versions/3.8.12/envs/2022_transformer/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# first tokenize all the values in the dataset\n",
    "data.values\n",
    "\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from collections import Counter, OrderedDict\n",
    "\n",
    "row_wise_data = data.values.astype('str').tolist()\n",
    "vocab = build_vocab_from_iterator(row_wise_data, specials = [\"<unk>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.transforms import VocabTransform, ToTensor, Sequential\n",
    "import torch.nn as nn\n",
    "\n",
    "# make vocab transform layer\n",
    "vocab_transform = VocabTransform(vocab)\n",
    "\n",
    "# get the length of the vocab and do word embedding\n",
    "word_embedding = nn.Embedding(vocab.__len__(), 128)\n",
    "\n",
    "# To tensor\n",
    "transform2tensor = ToTensor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change tokens to indices\n",
    "\n",
    "my_embedding_layer = Sequential(\n",
    "    vocab_transform,\n",
    "    transform2tensor\n",
    ")\n",
    "\n",
    "# change titanic data\n",
    "titanic_data = my_embedding_layer(row_wise_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 6])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "torch.randn(4,6, 8).mean(2).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "class extract_tensor(nn.Module):\n",
    "        def forward(self, x):\n",
    "            tensor, _ = x\n",
    "            return tensor\n",
    "\n",
    "class resizer(nn.Module):\n",
    "    def forward(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "        seq_len = x.shape[1]\n",
    "        return x.reshape(seq_len, batch_size, -1)\n",
    " \n",
    " \n",
    "class TableModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, \n",
    "                 input_size,\n",
    "                 hidden_size, \n",
    "                 n_layer,\n",
    "                 seq_len,\n",
    "                 vocab):\n",
    "        super().__init__()\n",
    "        \n",
    "        # embed vocab\n",
    "        self.vocab_size = vocab.__len__()\n",
    "        self.word_embedding = nn.Embedding(self.vocab_size, input_size)\n",
    "        self.resizer = resizer()\n",
    "        self.seq_len = seq_len\n",
    "        \n",
    "        self.embedding_layer = Sequential(\n",
    "            self.word_embedding,\n",
    "            self.resizer\n",
    "        )\n",
    "        \n",
    "        # bilstm layers\n",
    "        self.bilstm_layer1 = nn.LSTM(input_size=input_size,\n",
    "                                     hidden_size= hidden_size, \n",
    "                                     num_layers=n_layer,\n",
    "                                     bidirectional =True) \n",
    "        \n",
    "        self.bilstm_layer2 = nn.LSTM(input_size= hidden_size*2,\n",
    "                                     hidden_size= input_size//2,\n",
    "                                     num_layers = n_layer,\n",
    "                                     bidirectional=True)\n",
    "        \n",
    "        self.bilstm_layer3 = nn.LSTM(input_size =  input_size,\n",
    "                                     hidden_size = 1,\n",
    "                                     num_layers = 3,\n",
    "                                     bidirectional=True)\n",
    "        \n",
    "        self.final_layer = nn.Linear(self.seq_len, 1)\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            self.bilstm_layer1,\n",
    "            extract_tensor(),\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.ReLU(),\n",
    "            self.bilstm_layer2,\n",
    "            extract_tensor(),\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.ReLU(),\n",
    "            self.bilstm_layer3,\n",
    "            extract_tensor(),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.5),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.embedding_layer(x)\n",
    "        x = self.model(x)\n",
    "        x = torch.mean(x,2)\n",
    "        x = x.transpose(1,0)\n",
    "        x = self.final_layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_model = TableModel(128, 256, 2, 12, vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- What metric should I use to calculate loss??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset, TensorDataset\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, x): \n",
    "        self.x = x      \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        return self.x[idx]\n",
    "\n",
    "my_dataset = MyDataset(titanic_data)\n",
    "\n",
    "dataloader = DataLoader(my_dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1264,    1,    6, 1968,    5,   26,    2,    1,  959, 1529,    3,    4],\n",
      "        [1773,    1,    8, 1922,    5,   22,    1,    1, 2002,   23,    3,    4],\n",
      "        [1218,    2,    8, 2662,    7,   16,    8,    6,  168,  158,    3,    4],\n",
      "        [1220,    2,    8, 2405,    7,   83,    2,    8,  201,  175,    3,    9],\n",
      "        [1558,    2,    2, 2827,    5,   61,    2,    1,  243,   46,  361,    4],\n",
      "        [ 897,    2,    6, 2244,    7,   29,    2,    2,  173,  159,    3,    4],\n",
      "        [ 465,    1,    6, 2619,    5,   18,    1,    1, 2731,   36,    3,    4],\n",
      "        [1204,    1,    6, 2675,    5,   19,    2,    2,  300,  244,    3,    4],\n",
      "        [1566,    1,    6, 2359,    5,   34,    1,    1,  963,   37,    3,    4],\n",
      "        [ 565,    2,    6, 2357,    7,   68,    2,    2,  171,  149,    3,    4],\n",
      "        [1464,    2,    6, 2201,    5,    3,    1,    1,   71,   76,    3,    4],\n",
      "        [ 592,    2,    6, 1860,    7,   21,    2,    1, 1047,   48,    3,    4],\n",
      "        [1134,    1,    2, 2893,    5,   28,    1,    8,  446,  620, 2023,    9],\n",
      "        [ 646,    1,    6, 1944,    5,   20,    1,    1, 2595,  336,    3,    4],\n",
      "        [1398,    2,    2, 2808,    7,   46,    2,    1,  297,  330,  371,    9],\n",
      "        [1122,    1,    6, 2894,    5,   19,    2,    1,  849,  315,    3,    4],\n",
      "        [1405,    1,    6, 1959,    5,    3,    1,    1,  757,   51,    3,    9],\n",
      "        [1426,    2,    2, 2766,    7,   44,    1,    1,  198,  157,  360,    4],\n",
      "        [1604,    2,    6, 2498,    5,  179,    1,    2,  304,  109,  376,    4],\n",
      "        [ 593,    2,    8, 2528,    5,   83,    2,    2,  161,   11,  195,    4],\n",
      "        [ 645,    2,    2, 2328,    5,   52,    2,    1,  242,  142,  369,    4],\n",
      "        [1500,    1,    2, 2129,    5,   29,    2,    1, 2187,   46, 1904,    4],\n",
      "        [1412,    2,    2, 2156,    5,   90,    2,    1,  387,  311, 1828,    9],\n",
      "        [1411,    1,    6, 2248,    5,  104,   53,    8,   92,   89,    3,    4],\n",
      "        [1512,    1,    6, 2564,    5,   39,    1,    1, 1481,  335,    3,    4],\n",
      "        [1072,    1,    8, 2919,    5,   26,    2,    1, 2699,   94,    3,    9],\n",
      "        [ 605,    1,    8, 2912,    7,   16,    1,    1,  711,   12,    3,    4],\n",
      "        [1779,    2,    2, 2208,    7,   30,    6,    8,  116,  120,  144,    4],\n",
      "        [ 660,    1,    6, 2409,    5,   26,    1,    1,  966,   37,    3,    4],\n",
      "        [ 791,    1,    8, 2608,    5,    3,    1,    1,  162,   38,    3,    4],\n",
      "        [1525,    1,    6, 2382,    5,   59,    1,    1, 1077,   13,    3,    4],\n",
      "        [1304,    2,    6, 2122,    5,   26,    1,    1, 1142,   15,    3,   10]])\n"
     ]
    }
   ],
   "source": [
    "for data in dataloader:\n",
    "    print(data)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as f\n",
    "import torch.optim as optim\n",
    "\n",
    "def train_model(model, epoch):\n",
    "    \n",
    "    loss_fn = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "    \n",
    "    for e in range(0, epoch):\n",
    "        \n",
    "        for batch in dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            output = model(batch)\n",
    "            loss = loss_fn(output[0], batch.to(torch.float32))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            with torch.no_grad():\n",
    "                print(f'the loss is {loss}')\n",
    "        print(f'{e} epoch finished!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wonseok/.pyenv/versions/3.8.12/envs/2022_transformer/lib/python3.8/site-packages/torch/nn/modules/loss.py:529: UserWarning: Using a target size (torch.Size([32, 12])) that is different to the input size (torch.Size([1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(748929.9375, grad_fn=<MseLossBackward0>)\n",
      "the loss is 748929.9375\n",
      "tensor(798379.8125, grad_fn=<MseLossBackward0>)\n",
      "the loss is 798379.8125\n",
      "tensor(811125.3125, grad_fn=<MseLossBackward0>)\n",
      "the loss is 811125.3125\n",
      "tensor(757158.3125, grad_fn=<MseLossBackward0>)\n",
      "the loss is 757158.3125\n",
      "tensor(754493.6875, grad_fn=<MseLossBackward0>)\n",
      "the loss is 754493.6875\n",
      "tensor(813563.5625, grad_fn=<MseLossBackward0>)\n",
      "the loss is 813563.5625\n",
      "tensor(804478.8125, grad_fn=<MseLossBackward0>)\n",
      "the loss is 804478.8125\n",
      "tensor(799151.3125, grad_fn=<MseLossBackward0>)\n",
      "the loss is 799151.3125\n",
      "tensor(849818., grad_fn=<MseLossBackward0>)\n",
      "the loss is 849818.0\n",
      "tensor(830642.7500, grad_fn=<MseLossBackward0>)\n",
      "the loss is 830642.75\n",
      "tensor(817020.7500, grad_fn=<MseLossBackward0>)\n",
      "the loss is 817020.75\n",
      "tensor(890934.8125, grad_fn=<MseLossBackward0>)\n",
      "the loss is 890934.8125\n",
      "tensor(808585.2500, grad_fn=<MseLossBackward0>)\n",
      "the loss is 808585.25\n",
      "tensor(755185.3125, grad_fn=<MseLossBackward0>)\n",
      "the loss is 755185.3125\n",
      "tensor(759388.9375, grad_fn=<MseLossBackward0>)\n",
      "the loss is 759388.9375\n",
      "tensor(725879.9375, grad_fn=<MseLossBackward0>)\n",
      "the loss is 725879.9375\n",
      "tensor(677793.2500, grad_fn=<MseLossBackward0>)\n",
      "the loss is 677793.25\n",
      "tensor(793947.8125, grad_fn=<MseLossBackward0>)\n",
      "the loss is 793947.8125\n",
      "tensor(760036.4375, grad_fn=<MseLossBackward0>)\n",
      "the loss is 760036.4375\n",
      "tensor(765786.8125, grad_fn=<MseLossBackward0>)\n",
      "the loss is 765786.8125\n",
      "tensor(790456.1875, grad_fn=<MseLossBackward0>)\n",
      "the loss is 790456.1875\n",
      "tensor(680590.5625, grad_fn=<MseLossBackward0>)\n",
      "the loss is 680590.5625\n",
      "tensor(845669.1875, grad_fn=<MseLossBackward0>)\n",
      "the loss is 845669.1875\n",
      "tensor(715958.6875, grad_fn=<MseLossBackward0>)\n",
      "the loss is 715958.6875\n",
      "tensor(762995.7500, grad_fn=<MseLossBackward0>)\n",
      "the loss is 762995.75\n",
      "tensor(794337., grad_fn=<MseLossBackward0>)\n",
      "the loss is 794337.0\n",
      "tensor(755101.6875, grad_fn=<MseLossBackward0>)\n",
      "the loss is 755101.6875\n",
      "tensor(763639., grad_fn=<MseLossBackward0>)\n",
      "the loss is 763639.0\n",
      "{e} epoch finished!\n",
      "tensor(775237.0625, grad_fn=<MseLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wonseok/.pyenv/versions/3.8.12/envs/2022_transformer/lib/python3.8/site-packages/torch/nn/modules/loss.py:529: UserWarning: Using a target size (torch.Size([27, 12])) that is different to the input size (torch.Size([1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the loss is 775237.0625\n",
      "tensor(733205.3125, grad_fn=<MseLossBackward0>)\n",
      "the loss is 733205.3125\n",
      "tensor(761710.1875, grad_fn=<MseLossBackward0>)\n",
      "the loss is 761710.1875\n",
      "tensor(772098., grad_fn=<MseLossBackward0>)\n",
      "the loss is 772098.0\n",
      "tensor(772243.7500, grad_fn=<MseLossBackward0>)\n",
      "the loss is 772243.75\n",
      "tensor(783972.2500, grad_fn=<MseLossBackward0>)\n",
      "the loss is 783972.25\n",
      "tensor(749448.2500, grad_fn=<MseLossBackward0>)\n",
      "the loss is 749448.25\n",
      "tensor(884209.4375, grad_fn=<MseLossBackward0>)\n",
      "the loss is 884209.4375\n",
      "tensor(807358.5000, grad_fn=<MseLossBackward0>)\n",
      "the loss is 807358.5\n",
      "tensor(764711.8125, grad_fn=<MseLossBackward0>)\n",
      "the loss is 764711.8125\n",
      "tensor(818314.2500, grad_fn=<MseLossBackward0>)\n",
      "the loss is 818314.25\n",
      "tensor(767019., grad_fn=<MseLossBackward0>)\n",
      "the loss is 767019.0\n",
      "tensor(778660., grad_fn=<MseLossBackward0>)\n",
      "the loss is 778660.0\n",
      "tensor(801897.3125, grad_fn=<MseLossBackward0>)\n",
      "the loss is 801897.3125\n",
      "tensor(772680.8125, grad_fn=<MseLossBackward0>)\n",
      "the loss is 772680.8125\n",
      "tensor(778438.2500, grad_fn=<MseLossBackward0>)\n",
      "the loss is 778438.25\n",
      "tensor(776154.5000, grad_fn=<MseLossBackward0>)\n",
      "the loss is 776154.5\n",
      "tensor(796003.3125, grad_fn=<MseLossBackward0>)\n",
      "the loss is 796003.3125\n",
      "tensor(750542.2500, grad_fn=<MseLossBackward0>)\n",
      "the loss is 750542.25\n",
      "tensor(724234.7500, grad_fn=<MseLossBackward0>)\n",
      "the loss is 724234.75\n",
      "tensor(720740.2500, grad_fn=<MseLossBackward0>)\n",
      "the loss is 720740.25\n",
      "tensor(749723.8125, grad_fn=<MseLossBackward0>)\n",
      "the loss is 749723.8125\n",
      "tensor(833109.3125, grad_fn=<MseLossBackward0>)\n",
      "the loss is 833109.3125\n",
      "tensor(830707.7500, grad_fn=<MseLossBackward0>)\n",
      "the loss is 830707.75\n",
      "tensor(810642.5000, grad_fn=<MseLossBackward0>)\n",
      "the loss is 810642.5\n",
      "tensor(873102.5000, grad_fn=<MseLossBackward0>)\n",
      "the loss is 873102.5\n",
      "tensor(731064.8125, grad_fn=<MseLossBackward0>)\n",
      "the loss is 731064.8125\n",
      "tensor(696945.0625, grad_fn=<MseLossBackward0>)\n",
      "the loss is 696945.0625\n",
      "{e} epoch finished!\n",
      "tensor(777365.7500, grad_fn=<MseLossBackward0>)\n",
      "the loss is 777365.75\n",
      "tensor(828715.1875, grad_fn=<MseLossBackward0>)\n",
      "the loss is 828715.1875\n",
      "tensor(720711.5000, grad_fn=<MseLossBackward0>)\n",
      "the loss is 720711.5\n",
      "tensor(748060.1875, grad_fn=<MseLossBackward0>)\n",
      "the loss is 748060.1875\n",
      "tensor(747323.0625, grad_fn=<MseLossBackward0>)\n",
      "the loss is 747323.0625\n",
      "tensor(824545.0625, grad_fn=<MseLossBackward0>)\n",
      "the loss is 824545.0625\n",
      "tensor(732143.2500, grad_fn=<MseLossBackward0>)\n",
      "the loss is 732143.25\n",
      "tensor(832527.9375, grad_fn=<MseLossBackward0>)\n",
      "the loss is 832527.9375\n",
      "tensor(807730.5625, grad_fn=<MseLossBackward0>)\n",
      "the loss is 807730.5625\n",
      "tensor(804774.9375, grad_fn=<MseLossBackward0>)\n",
      "the loss is 804774.9375\n",
      "tensor(772656.8125, grad_fn=<MseLossBackward0>)\n",
      "the loss is 772656.8125\n",
      "tensor(830772.3125, grad_fn=<MseLossBackward0>)\n",
      "the loss is 830772.3125\n",
      "tensor(792505.0625, grad_fn=<MseLossBackward0>)\n",
      "the loss is 792505.0625\n",
      "tensor(748380.0625, grad_fn=<MseLossBackward0>)\n",
      "the loss is 748380.0625\n",
      "tensor(804360.8125, grad_fn=<MseLossBackward0>)\n",
      "the loss is 804360.8125\n",
      "tensor(817453.6875, grad_fn=<MseLossBackward0>)\n",
      "the loss is 817453.6875\n",
      "tensor(781480.4375, grad_fn=<MseLossBackward0>)\n",
      "the loss is 781480.4375\n",
      "tensor(730385.7500, grad_fn=<MseLossBackward0>)\n",
      "the loss is 730385.75\n",
      "tensor(771306.6875, grad_fn=<MseLossBackward0>)\n",
      "the loss is 771306.6875\n",
      "tensor(763383.5000, grad_fn=<MseLossBackward0>)\n",
      "the loss is 763383.5\n",
      "tensor(758261.9375, grad_fn=<MseLossBackward0>)\n",
      "the loss is 758261.9375\n",
      "tensor(763288.5000, grad_fn=<MseLossBackward0>)\n",
      "the loss is 763288.5\n",
      "tensor(747779.5625, grad_fn=<MseLossBackward0>)\n",
      "the loss is 747779.5625\n",
      "tensor(758793.1875, grad_fn=<MseLossBackward0>)\n",
      "the loss is 758793.1875\n",
      "tensor(766655.4375, grad_fn=<MseLossBackward0>)\n",
      "the loss is 766655.4375\n",
      "tensor(790668.5000, grad_fn=<MseLossBackward0>)\n",
      "the loss is 790668.5\n",
      "tensor(782940., grad_fn=<MseLossBackward0>)\n",
      "the loss is 782940.0\n",
      "tensor(822374.4375, grad_fn=<MseLossBackward0>)\n",
      "the loss is 822374.4375\n",
      "{e} epoch finished!\n",
      "tensor(816397.8125, grad_fn=<MseLossBackward0>)\n",
      "the loss is 816397.8125\n",
      "tensor(721764.2500, grad_fn=<MseLossBackward0>)\n",
      "the loss is 721764.25\n",
      "tensor(771059.6875, grad_fn=<MseLossBackward0>)\n",
      "the loss is 771059.6875\n",
      "tensor(803008.8125, grad_fn=<MseLossBackward0>)\n",
      "the loss is 803008.8125\n",
      "tensor(812912.1875, grad_fn=<MseLossBackward0>)\n",
      "the loss is 812912.1875\n",
      "tensor(778120.4375, grad_fn=<MseLossBackward0>)\n",
      "the loss is 778120.4375\n",
      "tensor(831271.8125, grad_fn=<MseLossBackward0>)\n",
      "the loss is 831271.8125\n",
      "tensor(819291.0625, grad_fn=<MseLossBackward0>)\n",
      "the loss is 819291.0625\n",
      "tensor(724461.6875, grad_fn=<MseLossBackward0>)\n",
      "the loss is 724461.6875\n",
      "tensor(789111.3125, grad_fn=<MseLossBackward0>)\n",
      "the loss is 789111.3125\n",
      "tensor(775250.8125, grad_fn=<MseLossBackward0>)\n",
      "the loss is 775250.8125\n",
      "tensor(823764.5625, grad_fn=<MseLossBackward0>)\n",
      "the loss is 823764.5625\n",
      "tensor(776628.8125, grad_fn=<MseLossBackward0>)\n",
      "the loss is 776628.8125\n",
      "tensor(826730.2500, grad_fn=<MseLossBackward0>)\n",
      "the loss is 826730.25\n",
      "tensor(755597., grad_fn=<MseLossBackward0>)\n",
      "the loss is 755597.0\n",
      "tensor(790468.5000, grad_fn=<MseLossBackward0>)\n",
      "the loss is 790468.5\n",
      "tensor(737588.1875, grad_fn=<MseLossBackward0>)\n",
      "the loss is 737588.1875\n",
      "tensor(773077.0625, grad_fn=<MseLossBackward0>)\n",
      "the loss is 773077.0625\n",
      "tensor(797883.5625, grad_fn=<MseLossBackward0>)\n",
      "the loss is 797883.5625\n",
      "tensor(777791.5000, grad_fn=<MseLossBackward0>)\n",
      "the loss is 777791.5\n",
      "tensor(756100.9375, grad_fn=<MseLossBackward0>)\n",
      "the loss is 756100.9375\n",
      "tensor(755647.3125, grad_fn=<MseLossBackward0>)\n",
      "the loss is 755647.3125\n",
      "tensor(743700.3125, grad_fn=<MseLossBackward0>)\n",
      "the loss is 743700.3125\n",
      "tensor(789326.9375, grad_fn=<MseLossBackward0>)\n",
      "the loss is 789326.9375\n",
      "tensor(741717.5000, grad_fn=<MseLossBackward0>)\n",
      "the loss is 741717.5\n",
      "tensor(731727.1875, grad_fn=<MseLossBackward0>)\n",
      "the loss is 731727.1875\n",
      "tensor(822992.3125, grad_fn=<MseLossBackward0>)\n",
      "the loss is 822992.3125\n",
      "tensor(769203.1875, grad_fn=<MseLossBackward0>)\n",
      "the loss is 769203.1875\n",
      "{e} epoch finished!\n",
      "tensor(821885.3125, grad_fn=<MseLossBackward0>)\n",
      "the loss is 821885.3125\n",
      "tensor(736843.6875, grad_fn=<MseLossBackward0>)\n",
      "the loss is 736843.6875\n",
      "tensor(764494.8125, grad_fn=<MseLossBackward0>)\n",
      "the loss is 764494.8125\n",
      "tensor(847033., grad_fn=<MseLossBackward0>)\n",
      "the loss is 847033.0\n",
      "tensor(786786.6875, grad_fn=<MseLossBackward0>)\n",
      "the loss is 786786.6875\n",
      "tensor(765662.3125, grad_fn=<MseLossBackward0>)\n",
      "the loss is 765662.3125\n",
      "tensor(786044., grad_fn=<MseLossBackward0>)\n",
      "the loss is 786044.0\n",
      "tensor(787717.7500, grad_fn=<MseLossBackward0>)\n",
      "the loss is 787717.75\n",
      "tensor(825708.0625, grad_fn=<MseLossBackward0>)\n",
      "the loss is 825708.0625\n",
      "tensor(777557.1875, grad_fn=<MseLossBackward0>)\n",
      "the loss is 777557.1875\n",
      "tensor(739822.1875, grad_fn=<MseLossBackward0>)\n",
      "the loss is 739822.1875\n",
      "tensor(804217.5000, grad_fn=<MseLossBackward0>)\n",
      "the loss is 804217.5\n",
      "tensor(761186.3125, grad_fn=<MseLossBackward0>)\n",
      "the loss is 761186.3125\n",
      "tensor(806149.5625, grad_fn=<MseLossBackward0>)\n",
      "the loss is 806149.5625\n",
      "tensor(788109.9375, grad_fn=<MseLossBackward0>)\n",
      "the loss is 788109.9375\n",
      "tensor(812410.4375, grad_fn=<MseLossBackward0>)\n",
      "the loss is 812410.4375\n",
      "tensor(817491.7500, grad_fn=<MseLossBackward0>)\n",
      "the loss is 817491.75\n",
      "tensor(682731.0625, grad_fn=<MseLossBackward0>)\n",
      "the loss is 682731.0625\n",
      "tensor(781361.0625, grad_fn=<MseLossBackward0>)\n",
      "the loss is 781361.0625\n",
      "tensor(729896.1875, grad_fn=<MseLossBackward0>)\n",
      "the loss is 729896.1875\n",
      "tensor(790693.1875, grad_fn=<MseLossBackward0>)\n",
      "the loss is 790693.1875\n",
      "tensor(776782.5625, grad_fn=<MseLossBackward0>)\n",
      "the loss is 776782.5625\n",
      "tensor(790500.5000, grad_fn=<MseLossBackward0>)\n",
      "the loss is 790500.5\n",
      "tensor(718981.8125, grad_fn=<MseLossBackward0>)\n",
      "the loss is 718981.8125\n",
      "tensor(760253.2500, grad_fn=<MseLossBackward0>)\n",
      "the loss is 760253.25\n",
      "tensor(773842.5000, grad_fn=<MseLossBackward0>)\n",
      "the loss is 773842.5\n",
      "tensor(810364.6875, grad_fn=<MseLossBackward0>)\n",
      "the loss is 810364.6875\n",
      "tensor(760238., grad_fn=<MseLossBackward0>)\n",
      "the loss is 760238.0\n",
      "{e} epoch finished!\n",
      "tensor(781349.0625, grad_fn=<MseLossBackward0>)\n",
      "the loss is 781349.0625\n",
      "tensor(786142.5000, grad_fn=<MseLossBackward0>)\n",
      "the loss is 786142.5\n",
      "tensor(786557.2500, grad_fn=<MseLossBackward0>)\n",
      "the loss is 786557.25\n",
      "tensor(838175.1875, grad_fn=<MseLossBackward0>)\n",
      "the loss is 838175.1875\n",
      "tensor(800803.5000, grad_fn=<MseLossBackward0>)\n",
      "the loss is 800803.5\n",
      "tensor(768186.0625, grad_fn=<MseLossBackward0>)\n",
      "the loss is 768186.0625\n",
      "tensor(784897.6875, grad_fn=<MseLossBackward0>)\n",
      "the loss is 784897.6875\n",
      "tensor(825781.6875, grad_fn=<MseLossBackward0>)\n",
      "the loss is 825781.6875\n",
      "tensor(813697.5000, grad_fn=<MseLossBackward0>)\n",
      "the loss is 813697.5\n",
      "tensor(764216.6875, grad_fn=<MseLossBackward0>)\n",
      "the loss is 764216.6875\n",
      "tensor(755461.3125, grad_fn=<MseLossBackward0>)\n",
      "the loss is 755461.3125\n",
      "tensor(766805.7500, grad_fn=<MseLossBackward0>)\n",
      "the loss is 766805.75\n",
      "tensor(785342.8125, grad_fn=<MseLossBackward0>)\n",
      "the loss is 785342.8125\n",
      "tensor(744825.8125, grad_fn=<MseLossBackward0>)\n",
      "the loss is 744825.8125\n",
      "tensor(747681.2500, grad_fn=<MseLossBackward0>)\n",
      "the loss is 747681.25\n",
      "tensor(733057.9375, grad_fn=<MseLossBackward0>)\n",
      "the loss is 733057.9375\n",
      "tensor(758421.5000, grad_fn=<MseLossBackward0>)\n",
      "the loss is 758421.5\n",
      "tensor(784861.4375, grad_fn=<MseLossBackward0>)\n",
      "the loss is 784861.4375\n",
      "tensor(813843.8125, grad_fn=<MseLossBackward0>)\n",
      "the loss is 813843.8125\n",
      "tensor(824612.6875, grad_fn=<MseLossBackward0>)\n",
      "the loss is 824612.6875\n",
      "tensor(809205., grad_fn=<MseLossBackward0>)\n",
      "the loss is 809205.0\n",
      "tensor(749930., grad_fn=<MseLossBackward0>)\n",
      "the loss is 749930.0\n",
      "tensor(758569.6875, grad_fn=<MseLossBackward0>)\n",
      "the loss is 758569.6875\n",
      "tensor(750218.7500, grad_fn=<MseLossBackward0>)\n",
      "the loss is 750218.75\n",
      "tensor(780891.6875, grad_fn=<MseLossBackward0>)\n",
      "the loss is 780891.6875\n",
      "tensor(816130.5625, grad_fn=<MseLossBackward0>)\n",
      "the loss is 816130.5625\n",
      "tensor(736934.5000, grad_fn=<MseLossBackward0>)\n",
      "the loss is 736934.5\n",
      "tensor(726462.4375, grad_fn=<MseLossBackward0>)\n",
      "the loss is 726462.4375\n",
      "{e} epoch finished!\n",
      "tensor(743109.3125, grad_fn=<MseLossBackward0>)\n",
      "the loss is 743109.3125\n",
      "tensor(767441., grad_fn=<MseLossBackward0>)\n",
      "the loss is 767441.0\n",
      "tensor(794089.4375, grad_fn=<MseLossBackward0>)\n",
      "the loss is 794089.4375\n",
      "tensor(805450.9375, grad_fn=<MseLossBackward0>)\n",
      "the loss is 805450.9375\n",
      "tensor(691411.5000, grad_fn=<MseLossBackward0>)\n",
      "the loss is 691411.5\n",
      "tensor(742429.4375, grad_fn=<MseLossBackward0>)\n",
      "the loss is 742429.4375\n",
      "tensor(822208.1875, grad_fn=<MseLossBackward0>)\n",
      "the loss is 822208.1875\n",
      "tensor(831777.5000, grad_fn=<MseLossBackward0>)\n",
      "the loss is 831777.5\n",
      "tensor(742204.7500, grad_fn=<MseLossBackward0>)\n",
      "the loss is 742204.75\n",
      "tensor(745846.5000, grad_fn=<MseLossBackward0>)\n",
      "the loss is 745846.5\n",
      "tensor(771452.8125, grad_fn=<MseLossBackward0>)\n",
      "the loss is 771452.8125\n",
      "tensor(770183.9375, grad_fn=<MseLossBackward0>)\n",
      "the loss is 770183.9375\n",
      "tensor(753239.3125, grad_fn=<MseLossBackward0>)\n",
      "the loss is 753239.3125\n",
      "tensor(759307.9375, grad_fn=<MseLossBackward0>)\n",
      "the loss is 759307.9375\n",
      "tensor(740875.8125, grad_fn=<MseLossBackward0>)\n",
      "the loss is 740875.8125\n",
      "tensor(824075.7500, grad_fn=<MseLossBackward0>)\n",
      "the loss is 824075.75\n",
      "tensor(782445.4375, grad_fn=<MseLossBackward0>)\n",
      "the loss is 782445.4375\n",
      "tensor(794567.5625, grad_fn=<MseLossBackward0>)\n",
      "the loss is 794567.5625\n",
      "tensor(799916.3125, grad_fn=<MseLossBackward0>)\n",
      "the loss is 799916.3125\n",
      "tensor(825734., grad_fn=<MseLossBackward0>)\n",
      "the loss is 825734.0\n",
      "tensor(813165.9375, grad_fn=<MseLossBackward0>)\n",
      "the loss is 813165.9375\n",
      "tensor(855880.1875, grad_fn=<MseLossBackward0>)\n",
      "the loss is 855880.1875\n",
      "tensor(735249.5625, grad_fn=<MseLossBackward0>)\n",
      "the loss is 735249.5625\n",
      "tensor(697022.8125, grad_fn=<MseLossBackward0>)\n",
      "the loss is 697022.8125\n",
      "tensor(813223.6875, grad_fn=<MseLossBackward0>)\n",
      "the loss is 813223.6875\n",
      "tensor(762645.9375, grad_fn=<MseLossBackward0>)\n",
      "the loss is 762645.9375\n",
      "tensor(853478.0625, grad_fn=<MseLossBackward0>)\n",
      "the loss is 853478.0625\n",
      "tensor(752228.3750, grad_fn=<MseLossBackward0>)\n",
      "the loss is 752228.375\n",
      "{e} epoch finished!\n",
      "tensor(785712.7500, grad_fn=<MseLossBackward0>)\n",
      "the loss is 785712.75\n",
      "tensor(779335.1875, grad_fn=<MseLossBackward0>)\n",
      "the loss is 779335.1875\n",
      "tensor(836481.3125, grad_fn=<MseLossBackward0>)\n",
      "the loss is 836481.3125\n",
      "tensor(793574.6875, grad_fn=<MseLossBackward0>)\n",
      "the loss is 793574.6875\n",
      "tensor(778779.7500, grad_fn=<MseLossBackward0>)\n",
      "the loss is 778779.75\n",
      "tensor(794931., grad_fn=<MseLossBackward0>)\n",
      "the loss is 794931.0\n",
      "tensor(764288.5625, grad_fn=<MseLossBackward0>)\n",
      "the loss is 764288.5625\n",
      "tensor(791258.5000, grad_fn=<MseLossBackward0>)\n",
      "the loss is 791258.5\n",
      "tensor(717059.5625, grad_fn=<MseLossBackward0>)\n",
      "the loss is 717059.5625\n",
      "tensor(807948.6875, grad_fn=<MseLossBackward0>)\n",
      "the loss is 807948.6875\n",
      "tensor(763392.5000, grad_fn=<MseLossBackward0>)\n",
      "the loss is 763392.5\n",
      "tensor(739150.1875, grad_fn=<MseLossBackward0>)\n",
      "the loss is 739150.1875\n",
      "tensor(758973., grad_fn=<MseLossBackward0>)\n",
      "the loss is 758973.0\n",
      "tensor(781701.6875, grad_fn=<MseLossBackward0>)\n",
      "the loss is 781701.6875\n",
      "tensor(758670.8125, grad_fn=<MseLossBackward0>)\n",
      "the loss is 758670.8125\n",
      "tensor(797551.5000, grad_fn=<MseLossBackward0>)\n",
      "the loss is 797551.5\n",
      "tensor(717817., grad_fn=<MseLossBackward0>)\n",
      "the loss is 717817.0\n",
      "tensor(729800.9375, grad_fn=<MseLossBackward0>)\n",
      "the loss is 729800.9375\n",
      "tensor(778973.5625, grad_fn=<MseLossBackward0>)\n",
      "the loss is 778973.5625\n",
      "tensor(877656.0625, grad_fn=<MseLossBackward0>)\n",
      "the loss is 877656.0625\n",
      "tensor(786250., grad_fn=<MseLossBackward0>)\n",
      "the loss is 786250.0\n",
      "tensor(723338.3125, grad_fn=<MseLossBackward0>)\n",
      "the loss is 723338.3125\n",
      "tensor(754843.1875, grad_fn=<MseLossBackward0>)\n",
      "the loss is 754843.1875\n",
      "tensor(773500.5000, grad_fn=<MseLossBackward0>)\n",
      "the loss is 773500.5\n",
      "tensor(762987.3125, grad_fn=<MseLossBackward0>)\n",
      "the loss is 762987.3125\n",
      "tensor(876925.1875, grad_fn=<MseLossBackward0>)\n",
      "the loss is 876925.1875\n",
      "tensor(754148.3125, grad_fn=<MseLossBackward0>)\n",
      "the loss is 754148.3125\n",
      "tensor(807901.3125, grad_fn=<MseLossBackward0>)\n",
      "the loss is 807901.3125\n",
      "{e} epoch finished!\n",
      "tensor(721624.3125, grad_fn=<MseLossBackward0>)\n",
      "the loss is 721624.3125\n",
      "tensor(676781.0625, grad_fn=<MseLossBackward0>)\n",
      "the loss is 676781.0625\n",
      "tensor(807633.2500, grad_fn=<MseLossBackward0>)\n",
      "the loss is 807633.25\n",
      "tensor(823698.6875, grad_fn=<MseLossBackward0>)\n",
      "the loss is 823698.6875\n",
      "tensor(731629.8125, grad_fn=<MseLossBackward0>)\n",
      "the loss is 731629.8125\n",
      "tensor(728544.8125, grad_fn=<MseLossBackward0>)\n",
      "the loss is 728544.8125\n",
      "tensor(781256.3125, grad_fn=<MseLossBackward0>)\n",
      "the loss is 781256.3125\n",
      "tensor(819464.0625, grad_fn=<MseLossBackward0>)\n",
      "the loss is 819464.0625\n",
      "tensor(743336.3125, grad_fn=<MseLossBackward0>)\n",
      "the loss is 743336.3125\n",
      "tensor(834625.1875, grad_fn=<MseLossBackward0>)\n",
      "the loss is 834625.1875\n",
      "tensor(744935.5000, grad_fn=<MseLossBackward0>)\n",
      "the loss is 744935.5\n",
      "tensor(757481.2500, grad_fn=<MseLossBackward0>)\n",
      "the loss is 757481.25\n",
      "tensor(792940.5625, grad_fn=<MseLossBackward0>)\n",
      "the loss is 792940.5625\n",
      "tensor(806492.6875, grad_fn=<MseLossBackward0>)\n",
      "the loss is 806492.6875\n",
      "tensor(806078., grad_fn=<MseLossBackward0>)\n",
      "the loss is 806078.0\n",
      "tensor(879377.7500, grad_fn=<MseLossBackward0>)\n",
      "the loss is 879377.75\n",
      "tensor(812048.9375, grad_fn=<MseLossBackward0>)\n",
      "the loss is 812048.9375\n",
      "tensor(836721.1875, grad_fn=<MseLossBackward0>)\n",
      "the loss is 836721.1875\n",
      "tensor(678801.6875, grad_fn=<MseLossBackward0>)\n",
      "the loss is 678801.6875\n",
      "tensor(690823.0625, grad_fn=<MseLossBackward0>)\n",
      "the loss is 690823.0625\n",
      "tensor(810576.1875, grad_fn=<MseLossBackward0>)\n",
      "the loss is 810576.1875\n",
      "tensor(788303.3125, grad_fn=<MseLossBackward0>)\n",
      "the loss is 788303.3125\n",
      "tensor(744474.6875, grad_fn=<MseLossBackward0>)\n",
      "the loss is 744474.6875\n",
      "tensor(772748.5625, grad_fn=<MseLossBackward0>)\n",
      "the loss is 772748.5625\n",
      "tensor(765386.2500, grad_fn=<MseLossBackward0>)\n",
      "the loss is 765386.25\n",
      "tensor(863233.6875, grad_fn=<MseLossBackward0>)\n",
      "the loss is 863233.6875\n",
      "tensor(816250.5625, grad_fn=<MseLossBackward0>)\n",
      "the loss is 816250.5625\n",
      "tensor(740795.5625, grad_fn=<MseLossBackward0>)\n",
      "the loss is 740795.5625\n",
      "{e} epoch finished!\n",
      "tensor(825293.5000, grad_fn=<MseLossBackward0>)\n",
      "the loss is 825293.5\n",
      "tensor(796922.5000, grad_fn=<MseLossBackward0>)\n",
      "the loss is 796922.5\n",
      "tensor(808092., grad_fn=<MseLossBackward0>)\n",
      "the loss is 808092.0\n",
      "tensor(820265.1875, grad_fn=<MseLossBackward0>)\n",
      "the loss is 820265.1875\n",
      "tensor(754303.2500, grad_fn=<MseLossBackward0>)\n",
      "the loss is 754303.25\n",
      "tensor(834297.3125, grad_fn=<MseLossBackward0>)\n",
      "the loss is 834297.3125\n",
      "tensor(783904.8125, grad_fn=<MseLossBackward0>)\n",
      "the loss is 783904.8125\n",
      "tensor(803235.9375, grad_fn=<MseLossBackward0>)\n",
      "the loss is 803235.9375\n",
      "tensor(753164.5000, grad_fn=<MseLossBackward0>)\n",
      "the loss is 753164.5\n",
      "tensor(779556.2500, grad_fn=<MseLossBackward0>)\n",
      "the loss is 779556.25\n",
      "tensor(800137.5000, grad_fn=<MseLossBackward0>)\n",
      "the loss is 800137.5\n",
      "tensor(728755.6875, grad_fn=<MseLossBackward0>)\n",
      "the loss is 728755.6875\n",
      "tensor(686619.6875, grad_fn=<MseLossBackward0>)\n",
      "the loss is 686619.6875\n",
      "tensor(761587.4375, grad_fn=<MseLossBackward0>)\n",
      "the loss is 761587.4375\n",
      "tensor(746482.1875, grad_fn=<MseLossBackward0>)\n",
      "the loss is 746482.1875\n",
      "tensor(729908.6875, grad_fn=<MseLossBackward0>)\n",
      "the loss is 729908.6875\n",
      "tensor(812933.5000, grad_fn=<MseLossBackward0>)\n",
      "the loss is 812933.5\n",
      "tensor(711913.6875, grad_fn=<MseLossBackward0>)\n",
      "the loss is 711913.6875\n",
      "tensor(786072.4375, grad_fn=<MseLossBackward0>)\n",
      "the loss is 786072.4375\n",
      "tensor(740783.8125, grad_fn=<MseLossBackward0>)\n",
      "the loss is 740783.8125\n",
      "tensor(811517.7500, grad_fn=<MseLossBackward0>)\n",
      "the loss is 811517.75\n",
      "tensor(816187.6875, grad_fn=<MseLossBackward0>)\n",
      "the loss is 816187.6875\n",
      "tensor(859489.6875, grad_fn=<MseLossBackward0>)\n",
      "the loss is 859489.6875\n",
      "tensor(778006.5625, grad_fn=<MseLossBackward0>)\n",
      "the loss is 778006.5625\n",
      "tensor(759233., grad_fn=<MseLossBackward0>)\n",
      "the loss is 759233.0\n",
      "tensor(795337.8125, grad_fn=<MseLossBackward0>)\n",
      "the loss is 795337.8125\n",
      "tensor(708337.5000, grad_fn=<MseLossBackward0>)\n",
      "the loss is 708337.5\n",
      "tensor(784086.9375, grad_fn=<MseLossBackward0>)\n",
      "the loss is 784086.9375\n",
      "{e} epoch finished!\n",
      "tensor(787715.7500, grad_fn=<MseLossBackward0>)\n",
      "the loss is 787715.75\n",
      "tensor(758480.8125, grad_fn=<MseLossBackward0>)\n",
      "the loss is 758480.8125\n",
      "tensor(866419.6875, grad_fn=<MseLossBackward0>)\n",
      "the loss is 866419.6875\n",
      "tensor(754383.1875, grad_fn=<MseLossBackward0>)\n",
      "the loss is 754383.1875\n",
      "tensor(742114.5625, grad_fn=<MseLossBackward0>)\n",
      "the loss is 742114.5625\n",
      "tensor(737859.2500, grad_fn=<MseLossBackward0>)\n",
      "the loss is 737859.25\n",
      "tensor(832423.8125, grad_fn=<MseLossBackward0>)\n",
      "the loss is 832423.8125\n",
      "tensor(792890.1875, grad_fn=<MseLossBackward0>)\n",
      "the loss is 792890.1875\n",
      "tensor(753106.3125, grad_fn=<MseLossBackward0>)\n",
      "the loss is 753106.3125\n",
      "tensor(780106.6875, grad_fn=<MseLossBackward0>)\n",
      "the loss is 780106.6875\n",
      "tensor(774078.8125, grad_fn=<MseLossBackward0>)\n",
      "the loss is 774078.8125\n",
      "tensor(794347.5625, grad_fn=<MseLossBackward0>)\n",
      "the loss is 794347.5625\n",
      "tensor(807467.0625, grad_fn=<MseLossBackward0>)\n",
      "the loss is 807467.0625\n",
      "tensor(768174.0625, grad_fn=<MseLossBackward0>)\n",
      "the loss is 768174.0625\n",
      "tensor(783000.3125, grad_fn=<MseLossBackward0>)\n",
      "the loss is 783000.3125\n",
      "tensor(700583.3125, grad_fn=<MseLossBackward0>)\n",
      "the loss is 700583.3125\n",
      "tensor(826816.4375, grad_fn=<MseLossBackward0>)\n",
      "the loss is 826816.4375\n",
      "tensor(780018.1875, grad_fn=<MseLossBackward0>)\n",
      "the loss is 780018.1875\n",
      "tensor(758399.8125, grad_fn=<MseLossBackward0>)\n",
      "the loss is 758399.8125\n",
      "tensor(773436.8125, grad_fn=<MseLossBackward0>)\n",
      "the loss is 773436.8125\n",
      "tensor(777562.0625, grad_fn=<MseLossBackward0>)\n",
      "the loss is 777562.0625\n",
      "tensor(787033.2500, grad_fn=<MseLossBackward0>)\n",
      "the loss is 787033.25\n",
      "tensor(719828.6875, grad_fn=<MseLossBackward0>)\n",
      "the loss is 719828.6875\n",
      "tensor(737723.0625, grad_fn=<MseLossBackward0>)\n",
      "the loss is 737723.0625\n",
      "tensor(792129.3125, grad_fn=<MseLossBackward0>)\n",
      "the loss is 792129.3125\n",
      "tensor(808058.1875, grad_fn=<MseLossBackward0>)\n",
      "the loss is 808058.1875\n",
      "tensor(732286.5000, grad_fn=<MseLossBackward0>)\n",
      "the loss is 732286.5\n",
      "tensor(854603.4375, grad_fn=<MseLossBackward0>)\n",
      "the loss is 854603.4375\n",
      "{e} epoch finished!\n",
      "tensor(785229.3125, grad_fn=<MseLossBackward0>)\n",
      "the loss is 785229.3125\n",
      "tensor(722957.8125, grad_fn=<MseLossBackward0>)\n",
      "the loss is 722957.8125\n",
      "tensor(799805., grad_fn=<MseLossBackward0>)\n",
      "the loss is 799805.0\n",
      "tensor(742932.9375, grad_fn=<MseLossBackward0>)\n",
      "the loss is 742932.9375\n",
      "tensor(771673.3125, grad_fn=<MseLossBackward0>)\n",
      "the loss is 771673.3125\n",
      "tensor(784241.2500, grad_fn=<MseLossBackward0>)\n",
      "the loss is 784241.25\n",
      "tensor(791004.3125, grad_fn=<MseLossBackward0>)\n",
      "the loss is 791004.3125\n",
      "tensor(730749.8125, grad_fn=<MseLossBackward0>)\n",
      "the loss is 730749.8125\n",
      "tensor(704428.6875, grad_fn=<MseLossBackward0>)\n",
      "the loss is 704428.6875\n",
      "tensor(844159.3125, grad_fn=<MseLossBackward0>)\n",
      "the loss is 844159.3125\n",
      "tensor(775508.3125, grad_fn=<MseLossBackward0>)\n",
      "the loss is 775508.3125\n",
      "tensor(685619.5000, grad_fn=<MseLossBackward0>)\n",
      "the loss is 685619.5\n",
      "tensor(803764.3125, grad_fn=<MseLossBackward0>)\n",
      "the loss is 803764.3125\n",
      "tensor(733590.0625, grad_fn=<MseLossBackward0>)\n",
      "the loss is 733590.0625\n",
      "tensor(742959.0625, grad_fn=<MseLossBackward0>)\n",
      "the loss is 742959.0625\n",
      "tensor(712924.9375, grad_fn=<MseLossBackward0>)\n",
      "the loss is 712924.9375\n",
      "tensor(780057.5000, grad_fn=<MseLossBackward0>)\n",
      "the loss is 780057.5\n",
      "tensor(794498., grad_fn=<MseLossBackward0>)\n",
      "the loss is 794498.0\n",
      "tensor(853478.0625, grad_fn=<MseLossBackward0>)\n",
      "the loss is 853478.0625\n",
      "tensor(821617.3125, grad_fn=<MseLossBackward0>)\n",
      "the loss is 821617.3125\n",
      "tensor(785286.8125, grad_fn=<MseLossBackward0>)\n",
      "the loss is 785286.8125\n",
      "tensor(747412.6875, grad_fn=<MseLossBackward0>)\n",
      "the loss is 747412.6875\n",
      "tensor(838986.4375, grad_fn=<MseLossBackward0>)\n",
      "the loss is 838986.4375\n",
      "tensor(760822.1875, grad_fn=<MseLossBackward0>)\n",
      "the loss is 760822.1875\n",
      "tensor(776710.0625, grad_fn=<MseLossBackward0>)\n",
      "the loss is 776710.0625\n",
      "tensor(854938.6875, grad_fn=<MseLossBackward0>)\n",
      "the loss is 854938.6875\n",
      "tensor(865923.8125, grad_fn=<MseLossBackward0>)\n",
      "the loss is 865923.8125\n",
      "tensor(746476.5000, grad_fn=<MseLossBackward0>)\n",
      "the loss is 746476.5\n",
      "{e} epoch finished!\n",
      "tensor(774150.5000, grad_fn=<MseLossBackward0>)\n",
      "the loss is 774150.5\n",
      "tensor(819387.5000, grad_fn=<MseLossBackward0>)\n",
      "the loss is 819387.5\n",
      "tensor(684630.8750, grad_fn=<MseLossBackward0>)\n",
      "the loss is 684630.875\n",
      "tensor(814184.4375, grad_fn=<MseLossBackward0>)\n",
      "the loss is 814184.4375\n",
      "tensor(719932.2500, grad_fn=<MseLossBackward0>)\n",
      "the loss is 719932.25\n",
      "tensor(801926., grad_fn=<MseLossBackward0>)\n",
      "the loss is 801926.0\n",
      "tensor(795866., grad_fn=<MseLossBackward0>)\n",
      "the loss is 795866.0\n",
      "tensor(824512.5000, grad_fn=<MseLossBackward0>)\n",
      "the loss is 824512.5\n",
      "tensor(771832.5000, grad_fn=<MseLossBackward0>)\n",
      "the loss is 771832.5\n",
      "tensor(738290.6875, grad_fn=<MseLossBackward0>)\n",
      "the loss is 738290.6875\n",
      "tensor(763256.8125, grad_fn=<MseLossBackward0>)\n",
      "the loss is 763256.8125\n",
      "tensor(753153.1875, grad_fn=<MseLossBackward0>)\n",
      "the loss is 753153.1875\n",
      "tensor(788568.3125, grad_fn=<MseLossBackward0>)\n",
      "the loss is 788568.3125\n",
      "tensor(774408.6875, grad_fn=<MseLossBackward0>)\n",
      "the loss is 774408.6875\n",
      "tensor(803164.4375, grad_fn=<MseLossBackward0>)\n",
      "the loss is 803164.4375\n",
      "tensor(861304.5625, grad_fn=<MseLossBackward0>)\n",
      "the loss is 861304.5625\n",
      "tensor(741012.1875, grad_fn=<MseLossBackward0>)\n",
      "the loss is 741012.1875\n",
      "tensor(744061.2500, grad_fn=<MseLossBackward0>)\n",
      "the loss is 744061.25\n",
      "tensor(741987.4375, grad_fn=<MseLossBackward0>)\n",
      "the loss is 741987.4375\n",
      "tensor(713568., grad_fn=<MseLossBackward0>)\n",
      "the loss is 713568.0\n",
      "tensor(745049.5625, grad_fn=<MseLossBackward0>)\n",
      "the loss is 745049.5625\n",
      "tensor(783505.5625, grad_fn=<MseLossBackward0>)\n",
      "the loss is 783505.5625\n",
      "tensor(841877.7500, grad_fn=<MseLossBackward0>)\n",
      "the loss is 841877.75\n",
      "tensor(813880.5000, grad_fn=<MseLossBackward0>)\n",
      "the loss is 813880.5\n",
      "tensor(879451.5625, grad_fn=<MseLossBackward0>)\n",
      "the loss is 879451.5625\n",
      "tensor(748329.2500, grad_fn=<MseLossBackward0>)\n",
      "the loss is 748329.25\n",
      "tensor(785928.1875, grad_fn=<MseLossBackward0>)\n",
      "the loss is 785928.1875\n",
      "tensor(720034.6875, grad_fn=<MseLossBackward0>)\n",
      "the loss is 720034.6875\n",
      "{e} epoch finished!\n",
      "tensor(818584., grad_fn=<MseLossBackward0>)\n",
      "the loss is 818584.0\n",
      "tensor(791987., grad_fn=<MseLossBackward0>)\n",
      "the loss is 791987.0\n",
      "tensor(805814.8125, grad_fn=<MseLossBackward0>)\n",
      "the loss is 805814.8125\n",
      "tensor(720058.6875, grad_fn=<MseLossBackward0>)\n",
      "the loss is 720058.6875\n",
      "tensor(844379.3125, grad_fn=<MseLossBackward0>)\n",
      "the loss is 844379.3125\n",
      "tensor(830843.8125, grad_fn=<MseLossBackward0>)\n",
      "the loss is 830843.8125\n",
      "tensor(740107.4375, grad_fn=<MseLossBackward0>)\n",
      "the loss is 740107.4375\n",
      "tensor(791557.1875, grad_fn=<MseLossBackward0>)\n",
      "the loss is 791557.1875\n",
      "tensor(729380.5000, grad_fn=<MseLossBackward0>)\n",
      "the loss is 729380.5\n",
      "tensor(750659.5000, grad_fn=<MseLossBackward0>)\n",
      "the loss is 750659.5\n",
      "tensor(847040.9375, grad_fn=<MseLossBackward0>)\n",
      "the loss is 847040.9375\n",
      "tensor(728531.7500, grad_fn=<MseLossBackward0>)\n",
      "the loss is 728531.75\n",
      "tensor(772561.5000, grad_fn=<MseLossBackward0>)\n",
      "the loss is 772561.5\n",
      "tensor(837106.8125, grad_fn=<MseLossBackward0>)\n",
      "the loss is 837106.8125\n",
      "tensor(786081.3125, grad_fn=<MseLossBackward0>)\n",
      "the loss is 786081.3125\n",
      "tensor(730585., grad_fn=<MseLossBackward0>)\n",
      "the loss is 730585.0\n",
      "tensor(756865.9375, grad_fn=<MseLossBackward0>)\n",
      "the loss is 756865.9375\n",
      "tensor(770497.1875, grad_fn=<MseLossBackward0>)\n",
      "the loss is 770497.1875\n",
      "tensor(746003.4375, grad_fn=<MseLossBackward0>)\n",
      "the loss is 746003.4375\n",
      "tensor(777793.3125, grad_fn=<MseLossBackward0>)\n",
      "the loss is 777793.3125\n",
      "tensor(813548.0625, grad_fn=<MseLossBackward0>)\n",
      "the loss is 813548.0625\n",
      "tensor(756743.5000, grad_fn=<MseLossBackward0>)\n",
      "the loss is 756743.5\n",
      "tensor(745868.5000, grad_fn=<MseLossBackward0>)\n",
      "the loss is 745868.5\n",
      "tensor(780189.1875, grad_fn=<MseLossBackward0>)\n",
      "the loss is 780189.1875\n",
      "tensor(821027.6875, grad_fn=<MseLossBackward0>)\n",
      "the loss is 821027.6875\n",
      "tensor(715643.8125, grad_fn=<MseLossBackward0>)\n",
      "the loss is 715643.8125\n",
      "tensor(751221.6875, grad_fn=<MseLossBackward0>)\n",
      "the loss is 751221.6875\n",
      "tensor(791340.5625, grad_fn=<MseLossBackward0>)\n",
      "the loss is 791340.5625\n",
      "{e} epoch finished!\n",
      "tensor(827519.4375, grad_fn=<MseLossBackward0>)\n",
      "the loss is 827519.4375\n",
      "tensor(745965., grad_fn=<MseLossBackward0>)\n",
      "the loss is 745965.0\n",
      "tensor(815747., grad_fn=<MseLossBackward0>)\n",
      "the loss is 815747.0\n",
      "tensor(786994.9375, grad_fn=<MseLossBackward0>)\n",
      "the loss is 786994.9375\n",
      "tensor(731860.7500, grad_fn=<MseLossBackward0>)\n",
      "the loss is 731860.75\n",
      "tensor(732760.2500, grad_fn=<MseLossBackward0>)\n",
      "the loss is 732760.25\n",
      "tensor(758052., grad_fn=<MseLossBackward0>)\n",
      "the loss is 758052.0\n",
      "tensor(790573.5000, grad_fn=<MseLossBackward0>)\n",
      "the loss is 790573.5\n",
      "tensor(779345.5000, grad_fn=<MseLossBackward0>)\n",
      "the loss is 779345.5\n",
      "tensor(785835.2500, grad_fn=<MseLossBackward0>)\n",
      "the loss is 785835.25\n",
      "tensor(799083.8125, grad_fn=<MseLossBackward0>)\n",
      "the loss is 799083.8125\n",
      "tensor(802628.7500, grad_fn=<MseLossBackward0>)\n",
      "the loss is 802628.75\n",
      "tensor(774875.1875, grad_fn=<MseLossBackward0>)\n",
      "the loss is 774875.1875\n",
      "tensor(735229.9375, grad_fn=<MseLossBackward0>)\n",
      "the loss is 735229.9375\n",
      "tensor(798314.3125, grad_fn=<MseLossBackward0>)\n",
      "the loss is 798314.3125\n",
      "tensor(734869.6875, grad_fn=<MseLossBackward0>)\n",
      "the loss is 734869.6875\n",
      "tensor(827639.5000, grad_fn=<MseLossBackward0>)\n",
      "the loss is 827639.5\n",
      "tensor(720631.6875, grad_fn=<MseLossBackward0>)\n",
      "the loss is 720631.6875\n",
      "tensor(840801.8125, grad_fn=<MseLossBackward0>)\n",
      "the loss is 840801.8125\n",
      "tensor(756460.8125, grad_fn=<MseLossBackward0>)\n",
      "the loss is 756460.8125\n",
      "tensor(818770.5000, grad_fn=<MseLossBackward0>)\n",
      "the loss is 818770.5\n",
      "tensor(757324.8125, grad_fn=<MseLossBackward0>)\n",
      "the loss is 757324.8125\n",
      "tensor(794700.0625, grad_fn=<MseLossBackward0>)\n",
      "the loss is 794700.0625\n",
      "tensor(761993.5000, grad_fn=<MseLossBackward0>)\n",
      "the loss is 761993.5\n",
      "tensor(782426., grad_fn=<MseLossBackward0>)\n",
      "the loss is 782426.0\n",
      "tensor(764834.1875, grad_fn=<MseLossBackward0>)\n",
      "the loss is 764834.1875\n",
      "tensor(770736.8125, grad_fn=<MseLossBackward0>)\n",
      "the loss is 770736.8125\n",
      "tensor(741960.1250, grad_fn=<MseLossBackward0>)\n",
      "the loss is 741960.125\n",
      "{e} epoch finished!\n",
      "tensor(776282.3125, grad_fn=<MseLossBackward0>)\n",
      "the loss is 776282.3125\n",
      "tensor(764340.6875, grad_fn=<MseLossBackward0>)\n",
      "the loss is 764340.6875\n",
      "tensor(760905.4375, grad_fn=<MseLossBackward0>)\n",
      "the loss is 760905.4375\n",
      "tensor(811808.0625, grad_fn=<MseLossBackward0>)\n",
      "the loss is 811808.0625\n",
      "tensor(758155.9375, grad_fn=<MseLossBackward0>)\n",
      "the loss is 758155.9375\n",
      "tensor(735717., grad_fn=<MseLossBackward0>)\n",
      "the loss is 735717.0\n",
      "tensor(746845.2500, grad_fn=<MseLossBackward0>)\n",
      "the loss is 746845.25\n",
      "tensor(759272.8125, grad_fn=<MseLossBackward0>)\n",
      "the loss is 759272.8125\n",
      "tensor(780352., grad_fn=<MseLossBackward0>)\n",
      "the loss is 780352.0\n",
      "tensor(805327.5000, grad_fn=<MseLossBackward0>)\n",
      "the loss is 805327.5\n",
      "tensor(761309.5000, grad_fn=<MseLossBackward0>)\n",
      "the loss is 761309.5\n",
      "tensor(841266.6875, grad_fn=<MseLossBackward0>)\n",
      "the loss is 841266.6875\n",
      "tensor(794244.5000, grad_fn=<MseLossBackward0>)\n",
      "the loss is 794244.5\n",
      "tensor(743730.9375, grad_fn=<MseLossBackward0>)\n",
      "the loss is 743730.9375\n",
      "tensor(794564.9375, grad_fn=<MseLossBackward0>)\n",
      "the loss is 794564.9375\n",
      "tensor(774338.2500, grad_fn=<MseLossBackward0>)\n",
      "the loss is 774338.25\n",
      "tensor(847402., grad_fn=<MseLossBackward0>)\n",
      "the loss is 847402.0\n",
      "tensor(740438., grad_fn=<MseLossBackward0>)\n",
      "the loss is 740438.0\n",
      "tensor(775208.0625, grad_fn=<MseLossBackward0>)\n",
      "the loss is 775208.0625\n",
      "tensor(731945.7500, grad_fn=<MseLossBackward0>)\n",
      "the loss is 731945.75\n",
      "tensor(735631.6875, grad_fn=<MseLossBackward0>)\n",
      "the loss is 735631.6875\n",
      "tensor(770102.5000, grad_fn=<MseLossBackward0>)\n",
      "the loss is 770102.5\n",
      "tensor(856040.5625, grad_fn=<MseLossBackward0>)\n",
      "the loss is 856040.5625\n",
      "tensor(768267.8125, grad_fn=<MseLossBackward0>)\n",
      "the loss is 768267.8125\n",
      "tensor(846763.5000, grad_fn=<MseLossBackward0>)\n",
      "the loss is 846763.5\n",
      "tensor(732063.3125, grad_fn=<MseLossBackward0>)\n",
      "the loss is 732063.3125\n",
      "tensor(807791.5000, grad_fn=<MseLossBackward0>)\n",
      "the loss is 807791.5\n",
      "tensor(705808.5625, grad_fn=<MseLossBackward0>)\n",
      "the loss is 705808.5625\n",
      "{e} epoch finished!\n",
      "tensor(817577.6875, grad_fn=<MseLossBackward0>)\n",
      "the loss is 817577.6875\n",
      "tensor(735260.3125, grad_fn=<MseLossBackward0>)\n",
      "the loss is 735260.3125\n",
      "tensor(804871.5625, grad_fn=<MseLossBackward0>)\n",
      "the loss is 804871.5625\n",
      "tensor(743099.7500, grad_fn=<MseLossBackward0>)\n",
      "the loss is 743099.75\n",
      "tensor(737222.1875, grad_fn=<MseLossBackward0>)\n",
      "the loss is 737222.1875\n",
      "tensor(878548.6875, grad_fn=<MseLossBackward0>)\n",
      "the loss is 878548.6875\n",
      "tensor(764964., grad_fn=<MseLossBackward0>)\n",
      "the loss is 764964.0\n",
      "tensor(787186., grad_fn=<MseLossBackward0>)\n",
      "the loss is 787186.0\n",
      "tensor(759045.6875, grad_fn=<MseLossBackward0>)\n",
      "the loss is 759045.6875\n",
      "tensor(771652.2500, grad_fn=<MseLossBackward0>)\n",
      "the loss is 771652.25\n",
      "tensor(754625.3125, grad_fn=<MseLossBackward0>)\n",
      "the loss is 754625.3125\n",
      "tensor(807393.5000, grad_fn=<MseLossBackward0>)\n",
      "the loss is 807393.5\n",
      "tensor(836368.7500, grad_fn=<MseLossBackward0>)\n",
      "the loss is 836368.75\n",
      "tensor(727532.5000, grad_fn=<MseLossBackward0>)\n",
      "the loss is 727532.5\n",
      "tensor(660150.0625, grad_fn=<MseLossBackward0>)\n",
      "the loss is 660150.0625\n",
      "tensor(850633.3125, grad_fn=<MseLossBackward0>)\n",
      "the loss is 850633.3125\n",
      "tensor(790986.0625, grad_fn=<MseLossBackward0>)\n",
      "the loss is 790986.0625\n",
      "tensor(772408.6875, grad_fn=<MseLossBackward0>)\n",
      "the loss is 772408.6875\n",
      "tensor(653206.1250, grad_fn=<MseLossBackward0>)\n",
      "the loss is 653206.125\n",
      "tensor(776894.6875, grad_fn=<MseLossBackward0>)\n",
      "the loss is 776894.6875\n",
      "tensor(793827.2500, grad_fn=<MseLossBackward0>)\n",
      "the loss is 793827.25\n",
      "tensor(740777.2500, grad_fn=<MseLossBackward0>)\n",
      "the loss is 740777.25\n",
      "tensor(809136.5625, grad_fn=<MseLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "train_model(my_model, 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequence to Sequence\n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a2a8cca9f7ae2054f045beba9ed73ad6f832c9901de5708c28ae510697f529c7"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('2022_transformer')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
